using QuantumCollocation
using NamedTrajectories
using TrajectoryIndexingUtils
using Flux
using ReinforcementLearning
using IntervalSets
using LinearAlgebra
using Base
using Distributions
using Statistics
using Printf
using Reexport
using Revise
using DomainSets
using Unicode
using IterTools

###Local L2 loss
function L2_loss(traj::NamedTrajectory{Float64},symb::Symbol,index::Int64,R::Float64;value::Union{Vector{Float64},Nothing}= nothing)
    return isnothing(value) ? R*traj.timestep^2/2 * sum(traj[symb][:,index].^2) : R*traj.timestep^2/2 * sum((traj[symb][:,index]-value).^2)  
end

function L2_loss(traj::NamedTrajectory{Float64},symb::Symbol,index::Vector{Int64},R::Float64;value::Union{Matrix{Float64},Nothing}= nothing)
    return isnothing(value) ? R*traj.timestep^2/2 * sum(traj[symb][:,index].^2) : R*traj.timestep^2/2 * sum((traj[symb][:,index]-value).^2)  
end

###Full Enviorment Definition of (Pre)Training Enviorments

function update!(traj::NamedTrajectory, name::Symbol, data::AbstractMatrix{Float64})
    # @assert name âˆˆ traj.names
    # @assert size(data, 1) == traj.dims[name]
    # @assert size(data, 2) == traj.T
    # TODO: test to see if updating both matrix and vec is necessary
    traj.data[traj.components[name], :] = data
    traj.datavec = vec(view(traj.data, :, :))
    return nothing
end

function update!(traj::NamedTrajectory, name::Symbol, idx::Int64,data::AbstractVector{Float64})
    current = deepcopy(traj[name])
    current[:,idx] = data
    update!(traj, name, current)
    return nothing
end
###Struct
Base.@kwdef mutable struct GatePretrainingEnv <: AbstractEnv
            system::AbstractQuantumSystem
            T::Int64
            ğ’¢::Gate
            N::Int64
            pretraining_trajectory::NamedTrajectory{Float64}
    
            dda_bound::Float64=1.0
            time_step::Float64=1/T
            
            traj::NamedTrajectory{Float64}
            Ï•âƒ—::Vector{Float64} = [range(0,2*pi,N)[i] for i in rand(DiscreteUniform(1,N),ğ’¢.n)]
end


###Construction from dynamics
function GatePretrainingEnv(system::AbstractQuantumSystem,T::Int64,ğ’¢::Gate,Î”t::Float64,N::Int64,pretraining_trajectory::NamedTrajectory{Float64};dda_bound::Float64=1.0)
    n_controls = length(system.H_drives)

    components = (
        a = Matrix{Float64}(zeros(n_controls,T)),
        da = Matrix{Float64}(zeros(n_controls,T)),
        dda =  Matrix{Float64}(zeros(n_controls,T)),
        UÌƒâƒ— = reduce(hcat,[operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1)))) for i in 1:T])
    )
    
    traj = NamedTrajectory(components; timestep=Î”t, controls=:a)
    return GatePretrainingEnv(
            system = system,
            T = T,
            ğ’¢ = ğ’¢,
            N = N,
            pretraining_trajectory = pretraining_trajectory,
            dda_bound = dda_bound,
            traj = traj
            )
end

###Struct
Base.@kwdef mutable struct GateTrainingEnv <: AbstractEnv
            system::AbstractQuantumSystem
            T::Int
            ğ’¢::Gate
    
            dda_bound::Float64=1.0
            time_step::Float64=1/T
            
            traj::NamedTrajectory{Float64}
            Ï•âƒ—::Vector{Float64} = [range(0,2*pi,N)[i] for i in rand(DiscreteUniform(1,N),ğ’¢.n)]
end

###Construction from dynamics
function GateTrainingEnv(system::AbstractQuantumSystem,T::Int64,ğ’¢::Gate,Î”t::Float64;dda_bound::Float64=1.0)
    n_controls = length(system.H_drives)

    components = (
        a = Matrix{Float64}(zeros(n_controls,T)),
        da = Matrix{Float64}(zeros(n_controls,T)),
        dda =  Matrix{Float64}(zeros(n_controls,T)),
        UÌƒâƒ— = reduce(hcat,[operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1)))) for i in 1:T])
    )
    
    traj = NamedTrajectory(components; timestep=Î”t, controls=:a)
    
    return GateTrainingEnv(
            system = system,
            T = T,
            ğ’¢ = ğ’¢,
            dda_bound = dda_bound,
            traj = traj
            )
end

#### (Shared) Env Atributes
RLBase.is_terminated(env::Union{GatePretrainingEnv,GateTrainingEnv}) = env.time_step >= (env.T-2)/env.T

RLBase.action_space(env::Union{GatePretrainingEnv,GateTrainingEnv}) = reduce(Ã—,[(-1..1) for i in 1:length(env.system.H_drives)])

RLBase.state_space(env::Union{GatePretrainingEnv,GateTrainingEnv}) = reduce(Ã—, [(-1..1) for i in 1:length(env.traj[:UÌƒâƒ—][:,1])]) Ã— reduce(Ã—, [(-Inf..Inf) for i in 1:2*length(env.system.H_drives)]) Ã— (1/env.T..1) Ã— reduce(Ã—,[(0..2*pi) for i in 1:env.ğ’¢.n])

RLBase.state(env::Union{GatePretrainingEnv,GateTrainingEnv})= reduce(vcat,[env.traj[:UÌƒâƒ—][:,Int64(round(env.time_step*env.T))],env.traj[:da][:,Int64(round(env.time_step*env.T))],env.traj[:a][:,Int64(round(env.time_step*env.T))],[env.time_step],env.Ï•âƒ—])


###Reset Functions
function RLBase.act!(env::Union{GatePretrainingEnv,GateTrainingEnv}, action::Union{Vector{Float32},Vector{Float64}})
    t = Int64(round(env.time_step*env.T))
    action = Vector{Float64}(action)*env.dda_bound

    
    update!(env.traj, :dda, t,action)
    update!(env.traj, :a, t+1,env.traj[:a][:,t] + env.traj[:da][:,t]*env.traj.timestep)
    update!(env.traj, :da, t+1, env.traj[:da][:,t] + env.traj[:dda][:,t]*env.traj.timestep)
    update!(env.traj, :UÌƒâƒ—, t+1,  unitary_rollout(env.traj[:UÌƒâƒ—][:,t],hcat(env.traj[:a][:,t],zeros(length(action))),env.traj.timestep,env.system)[:,end])
    
    env.time_step += 1/env.T

    if(RLBase.is_terminated(env))
        da0 = env.traj[:da][:,t+1]
        a0 = env.traj[:a][:,t+1]
        
        dda0 =  (-a0-da0*2*env.traj.timestep)/env.traj.timestep^2
        dda1 =  (-da0-dda0*env.traj.timestep)/env.traj.timestep
        
        update!(env.traj, :dda,t+1,dda0)
        update!(env.traj, :a,  t+2,env.traj[:a][:,t+1] + env.traj[:da][:,t+1]*env.traj.timestep)
        update!(env.traj, :da, t+2,env.traj[:da][:,t+1] + env.traj[:dda][:,t+1]*env.traj.timestep)
        update!(env.traj, :UÌƒâƒ—, t+2,unitary_rollout(env.traj[:UÌƒâƒ—][:,t+1],hcat(env.traj[:a][:,t+1],zeros(length(action))),env.traj.timestep,env.system)[:,end])

        update!(env.traj, :dda,t+2,dda1)
        update!(env.traj, :a,  t+3,env.traj[:a][:,t+2] + env.traj[:da][:,t+2]*env.traj.timestep)
        update!(env.traj, :da, t+3,env.traj[:da][:,t+2] + env.traj[:dda][:,t+2]*env.traj.timestep)
        update!(env.traj, :UÌƒâƒ—, t+3,unitary_rollout(env.traj[:UÌƒâƒ—][:,t+2],hcat(env.traj[:a][:,t+2],zeros(length(action))),env.traj.timestep,env.system)[:,end])

    end
end


###Reset funcs
function RLBase.reset!(env::GatePretrainingEnv; Ï•âƒ—::Union{Vector{Float64},Nothing}=nothing)
    env.time_step=1/env.T

    n_controls = length(system.H_drives)
    
    update!(env.traj,:a,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:da,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:dda,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:UÌƒâƒ—,reduce(hcat,[operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1)))) for i in 1:env.T]))
    
    env.Ï•âƒ— = isnothing(Ï•âƒ—) ? [range(0,2*pi,env.N)[i] for i in rand(DiscreteUniform(1,env.N),env.ğ’¢.n)] : Ï•âƒ—
end

function RLBase.reset!(env::GateTrainingEnv; Ï•âƒ—::Union{Vector{Float64},Nothing}=nothing)
    env.time_step=1/env.T

    n_controls = length(system.H_drives)
    
    update!(env.traj,:a,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:da,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:dda,Matrix{Float64}(zeros(n_controls,env.T)))
    update!(env.traj,:UÌƒâƒ—,reduce(hcat,[operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1)))) for i in 1:env.T]))
    
    env.Ï•âƒ— = isnothing(Ï•âƒ—) ? rand(Uniform(0,2*pi),env.ğ’¢.n) : Ï•âƒ—
end

###Reward funcs
function RLBase.reward(env::GatePretrainingEnv,
                action::Union{Vector{Float32},Nothing};
                S::Float64=1e-2,
                Q::Float64=100.0,
                S_a::Float64=S,
                S_da::Float64=S,
                S_dda::Float64=S,
                S_U::Float64=S)
    idx = Vector{Int64}(env.Ï•âƒ—.*(env.N-1)/(2*pi).+1)
    idx = sum((idx[1:env.ğ’¢.n-1].-1).*[env.N^(env.ğ’¢.n-i) for i in 1:env.ğ’¢.n-1])+idx[end]
    if(! RLBase.is_terminated(env))
        t = Int64(round(env.time_step*env.T))
        action = Vector{Float64}(action)*env.dda_bound
        return- sum((action -env.pretraining_trajectory[Symbol("dda"*string(idx))][:,t]).^2)*Î”t^2/2 * S_dda
               - L2_loss(env.traj,:a,t,S_a; value = env.pretraining_trajectory[Symbol("a"*string(idx))][:,t])
               - L2_loss(env.traj,:da,t,S_da; value = env.pretraining_trajectory[Symbol("da"*string(idx))][:,t])
               - L2_loss(env.traj,:UÌƒâƒ—,t,S_U; value = env.pretraining_trajectory[Symbol(Unicode.normalize("UÌƒâƒ—"*string(idx)))][:,t])
    else
        return - Q * unitary_infidelity(operator_to_iso_vec(env.ğ’¢(env.Ï•âƒ—)),env.traj[:UÌƒâƒ—][:,end])
              - L2_loss(env.traj,:dda,[env.T-2,env.T-1,env.T],S_dda; value = env.pretraining_trajectory[Symbol("dda"*string(idx))][:,env.T-2:env.T])
               - L2_loss(env.traj,:a,[env.T-2,env.T-1,env.T],S_a; value = env.pretraining_trajectory[Symbol("a"*string(idx))][:,env.T-2:env.T])
            - L2_loss(env.traj,:da,[env.T-2,env.T-1,env.T],S_da; value = env.pretraining_trajectory[Symbol("da"*string(idx))][:,env.T-2:env.T])
             - L2_loss(env.traj,:UÌƒâƒ—,[env.T-2,env.T-1,env.T],S_da; value = env.pretraining_trajectory[Symbol(Unicode.normalize("UÌƒâƒ—"*string(idx)))][:,env.T-2:env.T])

    end
end


function RLBase.reward(env::GateTrainingEnv,
                action::Union{AbstractVector{Float32},Nothing};
                R::Float64=1e-2 * 2/env.traj.timestep^2,
                Q::Float64=100.0 * 2/env.traj.timestep^2,
                R_a::Float64=R,
                R_da::Float64=R,
                R_dda::Float64=R)

   if(! RLBase.is_terminated(env))
        t = Int64(round(env.time_step*env.T))
        action = Vector{Float64}(action)*env.dda_bound
        return (- sum((action).^2)*Î”t^2/2 * R_dda
               - L2_loss(env.traj,:a,t,R_a)
               - L2_loss(env.traj,:da,t,R_da))
    else 
        return (- L2_loss(env.traj,:dda,[env.T-2,env.T-1,env.T],R_dda)
               - L2_loss(env.traj,:a,[env.T-2,env.T-1,env.T],R_a)
               - L2_loss(env.traj,:da,[env.T-2,env.T-1,env.T],R_da)
               - Q * unitary_infidelity(operator_to_iso_vec(env.ğ’¢(env.Ï•âƒ—)),env.traj[:UÌƒâƒ—][:,end]))
    end
end

function get_policy_infid(env::Union{GatePretrainingEnv,GateTrainingEnv},ğ’«::ActorCriticPolicy,thetas::Tuple{Float64})
    RLBase.reset!(env;Ï•âƒ—=[t for t in thetas])
    while(! RLBase.is_terminated(env))
        action = ğ’«(env;deterministic=true)
        RLBase.act!(env,action) 
    end
    return unitary_infidelity(operator_to_iso_vec(env.ğ’¢(env.Ï•âƒ—)),env.traj[:UÌƒâƒ—][:,end])
end

function policy_sample(env::Union{GatePretrainingEnv,GateTrainingEnv},ğ’«::ActorCriticPolicy,resolution::Int64)
    
    DATA = zeros([resolution for i in 1:env.ğ’¢.n]...)
    
    for (i,theta) in enumerate(product([range(0,2*pi,resolution) for i in 1:env.ğ’¢.n]...))
        coords = [(Int(ceil(i/resolution^(env.ğ’¢.n-j)))-1)%resolution+1 for j in range(1,env.ğ’¢.n)]
        DATA[coords...] = get_policy_infid(env,ğ’«,theta)  
            
    end
    
return convert(Array{Float64,env.ğ’¢.n},DATA)
end
