{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4131df-7b9c-44ce-8f3e-22389cdb2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuantumCollocation\n",
    "using NamedTrajectories\n",
    "using TrajectoryIndexingUtils\n",
    "using Flux\n",
    "using ReinforcementLearning\n",
    "using IntervalSets\n",
    "using LinearAlgebra\n",
    "using Base\n",
    "using Distributions\n",
    "using Statistics\n",
    "using Printf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db38a22f-b849-4cb9-bd07-286c09573042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "euler (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Base.@kwdef mutable struct PretrainingGateEnv <: AbstractEnv\n",
    "            system::AbstractQuantumSystem\n",
    "            T::Int\n",
    "            g::Gate\n",
    "            Δt::Union{Float64, Vector{Float64}}\n",
    "            N::Int64\n",
    "            pretraining_trajectory::NamedTrajectory{Float64}\n",
    "    \n",
    "            dda_bound::Float64=1.0\n",
    "            current_op::AbstractVector{Float64} = operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1))))\n",
    "            time_step::Float64=1/T\n",
    "            \n",
    "            a::AbstractMatrix{Float64} = reduce(hcat,[[0. for i in 1:length(system.H_drives)]])\n",
    "            da::AbstractMatrix{Float64} = reduce(hcat,[[0. for i in 1:length(system.H_drives)]])\n",
    "            dda::AbstractMatrix{Float64} = Matrix{Float64}(reshape([],length(system.H_drives),0))\n",
    "            angle::Vector{Float64} = [range(0,2*pi,N)[i] for i in rand(DiscreteUniform(1,N),g.n)]\n",
    "\n",
    "end\n",
    "\n",
    "Base.@kwdef mutable struct TrainingGateEnv <: AbstractEnv\n",
    "            system::AbstractQuantumSystem\n",
    "            T::Int\n",
    "            g::Gate\n",
    "            Δt::Union{Float64, Vector{Float64}}\n",
    "    \n",
    "            dda_bound::Float64=1.0\n",
    "            current_op::AbstractVector{Float64} = operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1))))\n",
    "            time_step::Float64=1/T\n",
    "            \n",
    "            a::AbstractMatrix{Float64} = reduce(hcat,[[0. for i in 1:length(system.H_drives)]])\n",
    "            da::AbstractMatrix{Float64} = reduce(hcat,[[0. for i in 1:length(system.H_drives)]])\n",
    "            dda::AbstractMatrix{Float64} =  Matrix{Float64}(reshape([],length(system.H_drives),0))\n",
    "            angle::Vector{Float64} = rand(Uniform(0,2*pi),g.n)\n",
    "\n",
    "end\n",
    "\n",
    "RLBase.is_terminated(env::Union{PretrainingGateEnv,TrainingGateEnv}) = env.time_step >= (env.T-2)/env.T\n",
    "\n",
    "RLBase.action_space(env::Union{PretrainingGateEnv,TrainingGateEnv}) = reduce(×,[(-1..1) for i in 1:length(env.system.H_drives)])\n",
    "\n",
    "RLBase.state_space(env::Union{PretrainingGateEnv,TrainingGateEnv}) = reduce(×, [(-1..1) for i in 1:length(env.current_op)]) × reduce(×, [(-Inf..Inf) for i in 1:2*length(env.system.H_drives)]) × (1/env.T..1) × reduce(×,[(0..2*pi) for i in 1:env.g.n])\n",
    "\n",
    "RLBase.state(env::Union{PretrainingGateEnv,TrainingGateEnv})= Vector{Float32}(reduce(vcat,[env.current_op,env.da[:,end],env.a[:,end],[env.time_step],env.angle]))\n",
    "\n",
    "function RLBase.act!(env::Union{PretrainingGateEnv,TrainingGateEnv}, action::Vector{Float32})\n",
    "    action = Vector{Float64}(action)*env.dda_bound\n",
    "    env.dda = hcat(env.dda,action)\n",
    "    env.a = hcat(env.a, env.a[:,end] + env.da[:,end]*env.Δt)\n",
    "    env.da = hcat(env.da, env.da[:,end] + env.dda[:,end]*env.Δt)\n",
    "    \n",
    "    env.time_step += 1/env.T\n",
    "    env.current_op = unitary_rollout(env.current_op,hcat(env.a[:,end],zeros(length(action))),env.Δt,env.system)[:,end]\n",
    "\n",
    "    if(RLBase.is_terminated(env))\n",
    "        da0 = env.da[:,end]\n",
    "        a0 = env.a[:,end]\n",
    "        \n",
    "        dda0 = (-a0-da0*2*env.Δt)/env.Δt^2\n",
    "        env.dda = hcat(env.dda, dda0)\n",
    "        env.a = hcat(env.a, env.a[:,end] + env.da[:,end]*env.Δt)\n",
    "        env.da = hcat(env.da, env.da[:,end] + env.dda[:,end]*env.Δt)\n",
    "\n",
    "        dda1=(-da0-dda0*env.Δt)/env.Δt\n",
    "        env.dda = hcat(env.dda, dda1)\n",
    "        env.a = hcat(env.a, env.a[:,end] + env.da[:,end]*env.Δt)\n",
    "        env.da = hcat(env.da, env.da[:,end] + env.dda[:,end]*env.Δt)\n",
    "\n",
    "        env.dda = hcat(env.dda, [0. for i in 1:length(system.H_drives)])\n",
    "\n",
    "        env.current_op = unitary_rollout(env.current_op,hcat(env.a[:,end-1:end],zeros(length(action))),env.Δt,env.system)[:,end]\n",
    "\n",
    "    end\n",
    "end\n",
    "\n",
    "function RLBase.reset!(env::PretrainingGateEnv; angle::Union{Vector{Float64},Nothing}=nothing)\n",
    "    env.current_op = operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1))))\n",
    "    env.time_step=1/env.T\n",
    "    \n",
    "    env.a = reduce(hcat,[[0. for i in 1:length(env.system.H_drives)]])\n",
    "    env.da = reduce(hcat,[[0. for i in 1:length(env.system.H_drives)]])\n",
    "    env.dda =  Matrix{Float64}(reshape([],length(env.system.H_drives),0))\n",
    "    env.angle = isnothing(angle) ? [range(0,2*pi,N)[i] for i in rand(DiscreteUniform(1,N),env.g.n)] : angle\n",
    "end\n",
    "\n",
    "function RLBase.reset!(env::TrainingGateEnv; angle::Union{Vector{Float64},Nothing}=nothing)\n",
    "    env.current_op = operator_to_iso_vec(Matrix{ComplexF64}(I(size(system.H_drives[1], 1))))\n",
    "    env.time_step=1/env.T\n",
    "    \n",
    "    env.a = reduce(hcat,[[0. for i in 1:length(env.system.H_drives)]])\n",
    "    env.da = reduce(hcat,[[0. for i in 1:length(env.system.H_drives)]])\n",
    "    env.dda =  Matrix{Float64}(reshape([],length(env.system.H_drives),0))\n",
    "    env.angle = isnothing(angle) ? rand(Uniform(0,2*pi),env.g.n) : angle\n",
    "end\n",
    "\n",
    "struct GatePolicy\n",
    "    mean_network::Chain\n",
    "    std_network::Chain\n",
    "end\n",
    "\n",
    "function GatePolicy(env::Union{PretrainingGateEnv,TrainingGateEnv};l::Vector{Int64}=[16,16])\n",
    "    out = length(env.system.H_drives)\n",
    "    mean_in = length(RLBase.state(env))\n",
    "    std_in = env.g.n\n",
    "\n",
    "    mean_network = Chain(Dense(mean_in=>l[1],relu),[Dense(l[i]=>l[i+1],relu) for i in 1:length(l)-1]...,Dense(l[end]=>out,softsign))\n",
    "    std_network = Chain(Dense(std_in=>l[1],relu),[Dense(l[i]=>l[i+1],relu) for i in 1:length(l)-1]...,Dense(l[end]=>1))\n",
    "\n",
    "    return GatePolicy(mean_network,std_network)\n",
    "end\n",
    "\n",
    "function(Policy::GatePolicy)(env::Union{PretrainingGateEnv,TrainingGateEnv}; deterministic::Bool = false)\n",
    "    state = Vector{Float32}(RLBase.state(env))\n",
    "    means = Policy.mean_network(state)\n",
    "    if(!deterministic)\n",
    "        std = exp(Policy.std_network(state[end-env.g.n+1:end])[1])\n",
    "        return means+rand(Normal(0,std),length(means))\n",
    "    else\n",
    "        return means\n",
    "    end        \n",
    "end\n",
    "\n",
    "# function(Policy::GatePolicy)(state::Vector{Float32}; deterministic::Bool = false)\n",
    "#     means = Policy.mean_network(state)\n",
    "#     if(!deterministic)\n",
    "#         std = exp(Policy.std_network(state[end-env.g.n+1:end])[1])\n",
    "#         return means+rand(Normal(0,std),length(means))\n",
    "#     else\n",
    "#         return means\n",
    "#     end        \n",
    "# end\n",
    "\n",
    "function deepcopy(Policy::GatePolicy)\n",
    "    return GatePolicy(Flux.deepcopy(Policy.mean_network),Flux.deepcopy(Policy.std_network))\n",
    "end\n",
    "\n",
    "function policy_prob(policy::GatePolicy,state::Vector{Float32},action::Vector{Float32})\n",
    "    n=size(policy.std_network.layers[1].weight)[end]\n",
    "    means = policy.mean_network(state)\n",
    "    std = exp(policy.std_network(state[end-n+1:end])[1])\n",
    "    devs = action-means\n",
    "    return Float32(reduce(*,exp.(-devs.^2/(2*std^2))*1/sqrt(2 * pi * std^2)))\n",
    "end\n",
    "\n",
    "function policy_log_prob(policy::GatePolicy,state::Vector{Float32},action::Vector{Float32})\n",
    "    n=size(policy.std_network.layers[1].weight)[end]\n",
    "    means = policy.mean_network(state)\n",
    "    std = exp(policy.std_network(state[end-n+1:end])[1])\n",
    "    devs = action-means\n",
    "    return Float32(sum((-devs.^2/(2*std^2)).-1/2 * log(2 * pi * std^2)))\n",
    "end\n",
    "\n",
    "Flux.@functor GatePolicy\n",
    "\n",
    "function RLBase.reward(env::PretrainingGateEnv;\n",
    "                action::Union{AbstractVector{Float32},Nothing}=nothing,\n",
    "                S::Float64=2/(env.Δt)^2,\n",
    "                S_a::Float64=S,\n",
    "                S_da::Float64=S,\n",
    "                S_dda::Float64=S)\n",
    "    idx = Vector{Int64}(env.angle.*(env.N-1)/(2*pi).+1)\n",
    "    idx = sum((idx[1:env.g.n-1].-1).*[env.N^(env.g.n-i) for i in 1:env.g.n-1])+idx[end]\n",
    "    if(! RLBase.is_terminated(env))\n",
    "        t = Int64(round(env.time_step*env.T))\n",
    "        action = Vector{Float64}(action)*env.dda_bound\n",
    "        return -(sum((env.a[:,end] - env.pretraining_trajectory[Symbol(\"a\"*string(idx))][:,t]).^2)*Δt^2/2 * S_a+sum((env.da[:,end] -env.pretraining_trajectory[Symbol(\"da\"*string(idx))][:,t]).^2)*Δt^2/2 * S_da + sum((action -env.pretraining_trajectory[Symbol(\"dda\"*string(idx))][:,t]).^2)*Δt^2/2 * S_dda)\n",
    "    else\n",
    "        return -(sum((env.a[:,end-2:end] - env.pretraining_trajectory[Symbol(\"a\"*string(idx))][:,end-2:end]).^2)*Δt^2/2 * S_a\n",
    "            +sum((env.da[:,end-2:end] -env.pretraining_trajectory[Symbol(\"da\"*string(idx))][:,end-2:end]).^2)*Δt^2/2 * S_da \n",
    "            + sum((env.dda[:,end-2:end] -env.pretraining_trajectory[Symbol(\"dda\"*string(idx))][:,end-2:end]).^2)*Δt^2/2 * S_dda)\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function RLBase.reward(env::TrainingGateEnv;\n",
    "                action::Union{AbstractVector{Float32},Nothing}=nothing,\n",
    "                R::Float64=2/(env.Δt)^2,\n",
    "                Q::Float64=1e4 * 2/(env.Δt)^2,\n",
    "                R_a::Float64=R,\n",
    "                R_da::Float64=R,\n",
    "                R_dda::Float64=R)\n",
    "\n",
    "   if(! RLBase.is_terminated(env))\n",
    "        t = Int64(round(env.time_step*env.T))\n",
    "        action = Vector{Float64}(action)*env.dda_bound\n",
    "        return -(sum(env.a[:,end].^2)*Δt^2/2 * R_a+sum(env.da[:,end].^2)*Δt^2/2 * R_da + sum(action.^2)*Δt^2/2 * R_dda)\n",
    "    else \n",
    "        return -(sum(env.a[:,end-2:end].^2)*Δt^2/2 * R_a+sum(env.da[:,end-2:end].^2)*Δt^2/2 * R_da + sum(env.dda[:,end-2:end].^2)*Δt^2/2 * R_dda+Q*(1-abs(tr(iso_vec_to_operator(env.current_op)'env.g(env.angle))/size(env.system.H_drives[1])[1])))\n",
    "    end\n",
    "end\n",
    "\n",
    "function getTrajectoryLoss(env::PretrainingGateEnv;\n",
    "                S::Float64=2/(env.Δt)^2,\n",
    "                S_a::Float64=S,\n",
    "                S_da::Float64=S,\n",
    "                S_dda::Float64=S)\n",
    "    idx = Vector{Int64}(env.angle.*(env.N-1)/(2*pi).+1)\n",
    "    idx = sum((idx[1:env.g.n-1].-1).*[env.N^(env.g.n-i) for i in 1:env.g.n-1])+idx[end]\n",
    "    return -(sum((env.a - env.pretraining_trajectory[Symbol(\"a\"*string(idx))]).^2)*Δt^2/2 * S_a+sum((env.da -env.pretraining_trajectory[Symbol(\"da\"*string(idx))]).^2)*Δt^2/2 * S_da + sum((env.dda -env.pretraining_trajectory[Symbol(\"dda\"*string(idx))]).^2)*Δt^2/2 * S_dda)\n",
    "end\n",
    "\n",
    "function getTrajectoryLoss(env::TrainingGateEnv;\n",
    "                R::Float64=2/(env.Δt)^2,\n",
    "                Q::Float64=1e4*2/(env.Δt)^2,\n",
    "                R_a::Float64=R,\n",
    "                R_da::Float64=R,\n",
    "                R_dda::Float64=R)\n",
    "    \n",
    "    reg = sum(env.a.^2)*Δt^2/2 * R_a+sum(env.da.^2)*Δt^2/2 * R_da + sum(env.dda .^2)*Δt^2/2 * R_dda\n",
    "    return -(reg+Q*(1-abs(tr(iso_vec_to_operator(env.current_op)'env.g(env.angle))/size(env.system.H_drives[1])[1])))\n",
    "end\n",
    "\n",
    "function SampleTrajectory(Policy::GatePolicy,env::Union{PretrainingGateEnv,TrainingGateEnv};deterministic::Bool=false, kwargs...)\n",
    "    RLBase.reset!(env)\n",
    "    rewards = Vector{Float64}()\n",
    "    actions = Vector{Vector{Float32}}()\n",
    "    states  = Vector{Vector{Float64}}()\n",
    "    while(! RLBase.is_terminated(env))\n",
    "        push!(states,RLBase.state(env))\n",
    "        action = Policy(env;deterministic=deterministic)\n",
    "        push!(actions,action)\n",
    "        push!(rewards,RLBase.reward(env;action=action,kwargs...))\n",
    "        RLBase.act!(env,action) \n",
    "    end\n",
    "    rewards[end]+= RLBase.reward(env;kwargs...)\n",
    "    return Vector{Float32}(rewards),Vector{Vector{Float32}}(actions),Vector{Vector{Float32}}(states)\n",
    "end\n",
    "\n",
    "function euler(dda::Matrix{Float64},n_steps::Int64,Δt::Float64)\n",
    "    n_controls = size(dda)[1]\n",
    "    da_init=-sum(hcat([0 for i in 1:n_controls],cumsum(dda[:,1:end-1]*Δt,dims=2))[:,1:end-1],dims=2)/(n_steps-1)\n",
    "    da=hcat([0 for i in 1:n_controls],cumsum(dda[:,1:end-1]*Δt,dims=2)) + reduce(hcat,[da_init for i in 1:n_steps])\n",
    "    a_=hcat([0 for i in 1:n_controls],cumsum(da[:,1:end-1]*Δt,dims=2))\n",
    "    return a_\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e079ef4-c499-42e3-b033-650d04c6161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ_traj = load_traj(\"RZ_pretrained.jld2\")\n",
    "\n",
    "const Units = 1e9\n",
    "const MHz = 1e6 / Units\n",
    "const GHz = 1e9 / Units\n",
    "const ns = 1e-9 * Units\n",
    "const μs = 1e-6 * Units\n",
    ";\n",
    "\n",
    "\n",
    "# Operators\n",
    "const Paulis = Dict(\n",
    "    \"I\" => Matrix{ComplexF64}([1 0; 0 1]),\n",
    "    \"X\" => Matrix{ComplexF64}([0 1; 1 0]),\n",
    "    \"Y\" => Matrix{ComplexF64}([0 im; -im 0]),\n",
    "    \"Z\" => Matrix{ComplexF64}([1 0; 0 -1]),\n",
    ")\n",
    "\n",
    "rz_op(theta) = exp(-im/2 * theta[1] * Paulis[\"Z\"]);\n",
    "\n",
    "RZ = Gate(1,rz_op)\n",
    "\n",
    "H_drives = [\n",
    "     Paulis[\"X\"],Paulis[\"Y\"]\n",
    "]\n",
    "system = QuantumSystem(H_drives);\n",
    "t_f = 10* ns\n",
    "n_steps = 51\n",
    "times = range(0, t_f, n_steps)  # Alternative: collect(0:Δt:t_f)\n",
    "n_controls=1\n",
    "n_qubits=1;\n",
    "Δt = times[2] - times[1]\n",
    "\n",
    "N = 11\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8320b221-5186-4741-8f85-b56dcb42eda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GatePolicy(Chain(Dense(14 => 16, relu), Dense(16 => 16, relu), Dense(16 => 2, softsign)), Chain(Dense(1 => 16, relu), Dense(16 => 16, relu), Dense(16 => 1)))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pretraining_Env = PretrainingGateEnv(\n",
    "                                    system = system,\n",
    "                                    Δt=Δt,\n",
    "                                    T=n_steps,\n",
    "                                    g=RZ,\n",
    "                                    N=11,\n",
    "                                    pretraining_trajectory=RZ_traj;\n",
    "                                    dda_bound=1.0\n",
    "                                    )\n",
    "\n",
    "Training_Env = TrainingGateEnv(\n",
    "                            system = system,\n",
    "                            Δt=Δt,\n",
    "                            T=n_steps,\n",
    "                            g=RZ;\n",
    "                            dda_bound=1.0\n",
    "                            );\n",
    "\n",
    "policy = GatePolicy(Training_Env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bd932f6-aacf-46b6-bd4f-fdadd424ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards,acts,states = SampleTrajectory(policy,Pretraining_Env;)\n",
    "a=Pretraining_Env.a\n",
    "da=Pretraining_Env.da\n",
    "dda=Pretraining_Env.dda\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8640e96d-85e3-49f5-9dcb-38a47c314db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00782041106140241"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTrajectoryLoss(Pretraining_Env;)-sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07e4d246-a1f4-4a0e-9467-c1e975056bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4236188424877247e-28"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((euler(dda,n_steps,Δt)-a).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2801c67-9ff0-4a3c-aff6-8d87d800bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Float64}:\n",
       " -3.3306690738754696e-16\n",
       "  0.0\n",
       " -1.1102230246251565e-16\n",
       "  1.1102230246251565e-16\n",
       "  0.0\n",
       " -3.3306690738754696e-16\n",
       "  1.1102230246251565e-16\n",
       "  1.1102230246251565e-16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitary_rollout(operator_to_iso_vec([1+0.0im 0; 0 1]),a,Δt,system)[:,end]-Pretraining_Env.current_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35d123fe-f8ab-492a-b7d7-849cc5bac12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards,acts,states = SampleTrajectory(policy,Training_Env;)\n",
    "a=Training_Env.a\n",
    "da=Training_Env.da\n",
    "dda=Training_Env.dda\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1871a35b-7d35-45be-96eb-3f7adf20a19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007854093215428293"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getTrajectoryLoss(Training_Env;)-sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "108f672d-9728-4133-bca5-506565678c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7695618774468643e-28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((euler(dda,n_steps,Δt)-a).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e4b5347-dd8c-4ea2-b202-91bd85c6e17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Float64}:\n",
       " -1.1102230246251565e-16\n",
       " -3.3306690738754696e-16\n",
       "  4.718447854656915e-16\n",
       " -1.2906342661267445e-15\n",
       "  3.3306690738754696e-16\n",
       " -1.1102230246251565e-16\n",
       " -1.2906342661267445e-15\n",
       " -4.718447854656915e-16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitary_rollout(operator_to_iso_vec([1+0.0im 0; 0 1]),a,Δt,system)[:,end]-Training_Env.current_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b086ad1b-beea-4062-9bc6-2f18e508219d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ValueNetwork (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ValueNetwork(env::Union{PretrainingGateEnv,TrainingGateEnv};l::Vector{Int64}=[16,16])\n",
    "    input = length(RLBase.state(env))\n",
    "    return  Chain(Dense(input=>l[1],relu),[Dense(l[i]=>l[i+1],relu) for i in 1:length(l)-1]...,Dense(l[end]=>1))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1641286-9cfc-47c7-8550-2bc0ca1d03e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PPO (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_cumsum(l::Vector{Float32},γ::Float32) = [sum(l[j:end].*[γ^i for i in 0:length(l)-j]) for j in 1:length(l)]\n",
    "function GAE(states::Vector{Vector{Float32}},rewards::Vector{Float32},actions::Vector{Vector{Float32}},VNN::Chain;γ::Float32=Float32(0.99),λ::Float32=Float32(0.97))\n",
    "    vals = getindex.(VNN.(states),1)\n",
    "    push!(vals,vals[end])\n",
    "    δ = rewards[1:end] + γ * vals[2:end] - vals[1:end-1]\n",
    "    return discount_cumsum(rewards,γ),discount_cumsum(δ,γ*λ)\n",
    "end\n",
    "function FitValueVNN(VNN::Chain,states_list::Matrix{Float32},rewards_to_go::Matrix{Float32}; max_iter::Int64 = 1000, lr::Float32 = 1f-3,tol::Float32=1f-4, batchsize::Int64=32)\n",
    "    epoch_losses = Vector{Float32}()\n",
    "    loss(x, y) = mean(abs2.(x.- y))\n",
    "    opt_state = Flux.setup(Adam(lr), VNN)\n",
    "    data = Flux.DataLoader((states_list, rewards_to_go), batchsize=batchsize)\n",
    "    \n",
    "    for epoch in 1:max_iter\n",
    "        batch_losses = Vector{Float32}()\n",
    "        for (x_d,y_d) in data\n",
    "            val, grads = Flux.withgradient(VNN) do VNN\n",
    "              result = VNN(x_d)\n",
    "              loss(result, y_d)\n",
    "            end\n",
    "        \n",
    "            # Save the loss from the forward pass. (Done outside of gradient.)\n",
    "            push!(batch_losses, val)\n",
    "            Flux.update!(opt_state, VNN, grads[1])    \n",
    "        end\n",
    "        push!(epoch_losses, mean(batch_losses))\n",
    "        if(length(epoch_losses)>2 && abs(epoch_losses[end]-epoch_losses[end-1]) <= tol)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return epoch_losses\n",
    "end \n",
    "\n",
    "g(ϵ,A) = (A>=0) ? (1+ϵ) * A : (1-ϵ) * A\n",
    "\n",
    "function clip_optimize(policy::GatePolicy,\n",
    "                      rewards_to_go::Matrix{Float32},\n",
    "                      states_list::Matrix{Float32},\n",
    "                      acts_list::Matrix{Float32},\n",
    "                      Adv_list::Matrix{Float32};\n",
    "                      ϵ::Float32 = 1f-1,\n",
    "                      max_iter::Int64 = 1000, lr::Float32 = 1f-3,tol::Float32=1f-4, \n",
    "        targ_KL::Float32 = 1f-2,batchsize::Int64=32)\n",
    "    n = size(acts_list)[1]\n",
    "    old_policy = deepcopy(policy)\n",
    "    epoch_losses = Vector{Float32}()\n",
    "    opt_state = Flux.setup(Adam(lr), policy)\n",
    "    old_log_probs = reshape([policy_log_prob(old_policy,states_list[:,i],acts_list[:,i]) for i in 1:length(rewards_to_go)],size(Adv_list)...)\n",
    "    data = Flux.DataLoader((reduce(vcat,[rewards_to_go,Adv_list,old_log_probs]), vcat(states_list,acts_list)), batchsize=batchsize)\n",
    "    KL = 0\n",
    "    for epoch in 1:max_iter\n",
    "        batch_losses = Vector{Float32}()\n",
    "        for (x,y) in data\n",
    "            batch_rewards_to_go=x[1,:]\n",
    "            batch_Adv=x[2,:]\n",
    "            batch_old_log_probs=x[3,:]\n",
    "\n",
    "            batch_states = y[1:end-n,:]\n",
    "            batch_acts = y[end-n+1:end,:]\n",
    "\n",
    "            val, grads = Flux.withgradient(policy) do policy\n",
    "                -mean(minimum([exp.([policy_log_prob(policy,batch_states[:,i],batch_acts[:,i]) for i in 1:length(batch_rewards_to_go)].-batch_old_log_probs).*batch_Adv g.(ϵ,batch_Adv)],dims=2))\n",
    "            end\n",
    "            push!(batch_losses, val)\n",
    "            Flux.update!(opt_state, policy, grads[1])   \n",
    "        end\n",
    "        push!(epoch_losses, mean(batch_losses))\n",
    "        if(length(epoch_losses)>2 && abs(epoch_losses[end]-epoch_losses[end-1]) <= tol)\n",
    "            break\n",
    "        end   \n",
    "        new_log_probs = reshape([policy_log_prob(policy,states_list[:,i],acts_list[:,i]) for i in 1:length(rewards_to_go)],size(Adv_list)...)\n",
    "        log_ratio = new_log_probs.-old_log_probs\n",
    "        KL = mean((exp.(log_ratio).- 1).-log_ratio)\n",
    "        if((KL) >= 15f-1 * targ_KL)\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    return epoch_losses,KL\n",
    "end\n",
    "\n",
    "\n",
    "function PPO(env::Union{PretrainingGateEnv,TrainingGateEnv};\n",
    "             trajectory_batch_size::Int64=20, \n",
    "             epochs::Int64=100,\n",
    "             initial_policy::Union{GatePolicy,Nothing}=nothing,\n",
    "             initial_VNN::Union{Chain,Nothing}=nothing,\n",
    "             l::Vector{Int64}=[16,16,16],\n",
    "             max_iter::Int64 = 100, \n",
    "             vf_fit_lr::Float32 = 1f-2,\n",
    "             pi_fit_lr::Float32 = 3f-5,\n",
    "             fit_batch_size::Int64=32,\n",
    "             ϵ::Float32 = 2f-1,\n",
    "             verbose::Bool=true,\n",
    "             γ::Float32=Float32(0.99),\n",
    "             λ::Float32=Float32(0.97),\n",
    "            tol::Float32=Float32(-1),#1f-4,\n",
    "             trajectory_kwargs...)\n",
    "\n",
    "    policy = isnothing(initial_policy) ? GatePolicy(env;l=l) : initial_policy\n",
    "    VNN = isnothing(initial_VNN) ? ValueNetwork(env;l=l) : initial_VNN\n",
    "\n",
    "    for epoch in 1:epochs\n",
    "        rewards_to_go = Vector{Vector{Float32}}()\n",
    "        Adv_list =   Vector{Vector{Float32}}()\n",
    "        acts_list = Vector{Vector{Float32}}()\n",
    "        states_list = Vector{Vector{Float32}}()\n",
    "        for i in 1:trajectory_batch_size\n",
    "            rewards,acts,states = SampleTrajectory(policy,env;trajectory_kwargs...)\n",
    "            rtg,adv = GAE(states,rewards,acts,VNN;γ=γ,λ=λ)\n",
    "            \n",
    "            rewards_to_go = vcat(rewards_to_go,rtg)\n",
    "            Adv_list = vcat(Adv_list,adv)\n",
    "            \n",
    "            states_list=vcat(states_list,states)\n",
    "            acts_list=vcat(acts_list,acts)\n",
    "        end\n",
    "        \n",
    "        acts_list = Matrix{Float32}([acts_list[j][i] for i=1:size(acts_list[1])[1], j=1:size(acts_list)[1]])\n",
    "        states_list = Matrix{Float32}([states_list[j][i] for i=1:size(states_list[1])[1], j=1:size(states_list)[1]])\n",
    "        rewards_to_go =  Matrix{Float32}(reshape(rewards_to_go,1,length(rewards_to_go)))\n",
    "        Adv_list =  Matrix{Float32}(reshape(Adv_list,1,length(Adv_list)))\n",
    "        Adv_list = (Adv_list.-mean(Adv_list))/std(Adv_list)\n",
    "        \n",
    "        policy_losses,KL = clip_optimize(policy,rewards_to_go,states_list,acts_list,Adv_list;ϵ=ϵ,max_iter= max_iter, lr = pi_fit_lr, tol = tol, batchsize=fit_batch_size)\n",
    "        value_losses =FitValueVNN(VNN,states_list,rewards_to_go; max_iter= max_iter, lr= vf_fit_lr, tol = tol,batchsize=fit_batch_size)\n",
    "        if(verbose)\n",
    "            @printf \"Epoch %i Complete\\n\" epoch\n",
    "            @printf \"Mean Rtg: %.2f\\n\" mean(rewards_to_go)\n",
    "            @printf \"KL: %.2f\\n\" KL\n",
    "            @printf \"Policy Iters: %i\\n\" length(policy_losses)\n",
    "            @printf \"Value Iters: %i\\n\" length(value_losses)\n",
    "            @printf \"Value Loss 1: %.2f\\n\" value_losses[1]\n",
    "            @printf \"Value Loss end: %.2f\\n\" value_losses[end]\n",
    "\n",
    "            println(\"-------------------------\")\n",
    "            flush(stdout)\n",
    "        end\n",
    "    end\n",
    "    return policy,VNN\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ad953-861f-4018-b7a8-3e0acb5d0607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "policy,vnn = PPO(Pretraining_Env;epochs=250,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87688e8-f92e-4ed4-9b3d-c9fb51114381",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(0,2*pi,1000)\n",
    "y = [policy.std_network([v])[1] for v in x]\n",
    "using CairoMakie\n",
    "lines(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54116d9-d477-4f7b-a8cd-f14438f90db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RLBase.reset!(Pretraining_Env,angle = [range(0,2*pi,11)[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e114e-8b4c-4b9d-8e4e-0bb612279cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards,acts,states =SampleTrajectory(policy,Pretraining_Env;deterministic=true)\n",
    "rtg,adv = GAE(states,rewards,acts,vnn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40971cda-f784-41ea-805d-d4b99e698c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e99b95-6e2f-4b87-9ad0-b192a05cac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "getTrajectoryLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdc430-a85d-4cbe-bd20-1d5b1f1428dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d055d59-4f43-4780-a2aa-a5f359f703d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ_traj[:a5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fd2310-a116-4031-a0aa-06ce20fe0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1])\n",
    "lines!(ax,1:n_steps,Pretraining_Env.a[1,:])\n",
    "lines!(ax,1:n_steps,RZ_traj[:a5][1,:])\n",
    "lines!(ax,1:n_steps,abs.(RZ_traj[:a5][1,:]-Pretraining_Env.a[1,:]).^2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9289036-219e-4e40-b271-a44ec407d6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1])\n",
    "lines!(ax,1:n_steps,Pretraining_Env.da[1,:])\n",
    "lines!(ax,1:n_steps,RZ_traj[:da5][1,:])\n",
    "lines!(ax,1:n_steps,abs.(RZ_traj[:da5][1,:]-Pretraining_Env.da[1,:]).^2)\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df218b82-6a18-4030-8dca-77c3417f6c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
