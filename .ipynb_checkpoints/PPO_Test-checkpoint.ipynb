{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4131df-7b9c-44ce-8f3e-22389cdb2641",
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuantumCollocation\n",
    "using NamedTrajectories\n",
    "using TrajectoryIndexingUtils\n",
    "using Flux\n",
    "using ReinforcementLearning\n",
    "using IntervalSets\n",
    "using LinearAlgebra\n",
    "using Base\n",
    "using Distributions\n",
    "using Statistics\n",
    "using Printf\n",
    "using Reexport\n",
    "using Revise\n",
    "\n",
    "includet(\"RL-Copy1.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e079ef4-c499-42e3-b033-650d04c6161e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RZ_traj = load_traj(\"RZ_pretrained.jld2\")\n",
    "\n",
    "const Units = 1e9\n",
    "const MHz = 1e6 / Units\n",
    "const GHz = 1e9 / Units\n",
    "const ns = 1e-9 * Units\n",
    "const μs = 1e-6 * Units\n",
    ";\n",
    "\n",
    "\n",
    "# Operators\n",
    "const Paulis = Dict(\n",
    "    \"I\" => Matrix{ComplexF64}([1 0; 0 1]),\n",
    "    \"X\" => Matrix{ComplexF64}([0 1; 1 0]),\n",
    "    \"Y\" => Matrix{ComplexF64}([0 im; -im 0]),\n",
    "    \"Z\" => Matrix{ComplexF64}([1 0; 0 -1]),\n",
    ")\n",
    "\n",
    "rz_op(theta) = exp(-im/2 * theta[1] * Paulis[\"Z\"]);\n",
    "\n",
    "RZ = Gate(1,rz_op)\n",
    "\n",
    "H_drives = [\n",
    "     Paulis[\"X\"],Paulis[\"Y\"]\n",
    "]\n",
    "system = QuantumSystem(H_drives);\n",
    "t_f = 10* ns\n",
    "n_steps = 51\n",
    "times = range(0, t_f, n_steps)  # Alternative: collect(0:Δt:t_f)\n",
    "n_controls=1\n",
    "n_qubits=1;\n",
    "Δt = times[2] - times[1]\n",
    "\n",
    "N = 11\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8320b221-5186-4741-8f85-b56dcb42eda9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GatePolicy(Chain(Dense(14 => 16, relu), Dense(16 => 16, relu), Dense(16 => 2, softsign)), Chain(Dense(14 => 16, relu), Dense(16 => 16, relu), Dense(16 => 1, softsign)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pretraining_Env = PretrainingGateEnv(\n",
    "                                    system = system,\n",
    "                                    Δt=Δt,\n",
    "                                    T=n_steps,\n",
    "                                    g=RZ,\n",
    "                                    N=11,\n",
    "                                    pretraining_trajectory=RZ_traj;\n",
    "                                    dda_bound=1.0\n",
    "                                    )\n",
    "\n",
    "Training_Env = TrainingGateEnv(\n",
    "                            system = system,\n",
    "                            Δt=Δt,\n",
    "                            T=n_steps,\n",
    "                            g=RZ;\n",
    "                            dda_bound=1.0\n",
    "                            );\n",
    "\n",
    "policy = GatePolicy(Training_Env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd932f6-aacf-46b6-bd4f-fdadd424ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards,acts,states = SampleTrajectory(policy,Pretraining_Env)\n",
    "a=Pretraining_Env.a\n",
    "da=Pretraining_Env.da\n",
    "dda=Pretraining_Env.dda\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8640e96d-85e3-49f5-9dcb-38a47c314db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9935302866509661"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(getTrajectoryLoss(Pretraining_Env)-sum(rewards))/getTrajectoryLoss(Pretraining_Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07e4d246-a1f4-4a0e-9467-c1e975056bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.235445034508386e-28"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((euler(dda,n_steps,Δt)-a).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2801c67-9ff0-4a3c-aff6-8d87d800bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitary_rollout(operator_to_iso_vec([1+0.0im 0; 0 1]),a,Δt,system)[:,end]-Pretraining_Env.current_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d123fe-f8ab-492a-b7d7-849cc5bac12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards,acts,states = SampleTrajectory(policy,Training_Env)\n",
    "a=Training_Env.a\n",
    "da=Training_Env.da\n",
    "dda=Training_Env.dda\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1871a35b-7d35-45be-96eb-3f7adf20a19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929886496612557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(getTrajectoryLoss(Training_Env)-sum(rewards))/getTrajectoryLoss(Training_Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108f672d-9728-4133-bca5-506565678c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.979587176839717e-28"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((euler(dda,n_steps,Δt)-a).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e4b5347-dd8c-4ea2-b202-91bd85c6e17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8-element Vector{Float64}:\n",
       "  0.0\n",
       "  0.0\n",
       "  3.5822039778921066e-16\n",
       " -6.106226635438361e-16\n",
       "  0.0\n",
       "  0.0\n",
       " -6.106226635438361e-16\n",
       " -3.5822039778921066e-16"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unitary_rollout(operator_to_iso_vec([1+0.0im 0; 0 1]),a,Δt,system)[:,end]-Training_Env.current_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc2ad953-861f-4018-b7a8-3e0acb5d0607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations 1 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.70\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.37\n",
      "-------------------------\n",
      "Iterations 2 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.42\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 3 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 4 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 5 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 6 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.76\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.28\n",
      "-------------------------\n",
      "Iterations 7 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.19\n",
      "-------------------------\n",
      "Iterations 8 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.47\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 9 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.19\n",
      "-------------------------\n",
      "Iterations 10 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.69\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 11 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 12 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.54\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 13 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 14 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.73\n",
      "Final KL: 0.17\n",
      "Mean Policy Loss: 0.04\n",
      "Mean Value Loss: 0.36\n",
      "-------------------------\n",
      "Iterations 15 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 16 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 17 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 18 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.54\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.29\n",
      "-------------------------\n",
      "Iterations 19 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 20 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 21 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 22 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.80\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.39\n",
      "-------------------------\n",
      "Iterations 23 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.28\n",
      "-------------------------\n",
      "Iterations 24 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 25 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.69\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 26 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 27 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.70\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 28 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 29 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 30 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 31 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 32 Complete\n",
      "Epochs 8 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.23\n",
      "-------------------------\n",
      "Iterations 33 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 34 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.54\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 35 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 36 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 37 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 38 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.77\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.24\n",
      "-------------------------\n",
      "Iterations 39 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.50\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 40 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 41 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.50\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 42 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 43 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 44 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.56\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 45 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.50\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 46 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 47 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.74\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 48 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.62\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 49 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 50 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 51 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 52 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.78\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.21\n",
      "-------------------------\n",
      "Iterations 53 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 54 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 55 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.79\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 56 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.50\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.03\n",
      "-------------------------\n",
      "Iterations 57 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 58 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.49\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 59 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.43\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 60 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.54\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 61 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 62 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 63 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.49\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 64 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.70\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 65 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 66 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.62\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 67 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 68 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 69 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.80\n",
      "Final KL: 0.30\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 70 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.73\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 71 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 72 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 73 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.76\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 74 Complete\n",
      "Epochs 8 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 75 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 76 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 77 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.04\n",
      "-------------------------\n",
      "Iterations 78 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 79 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.80\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 80 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.70\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 81 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 82 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 83 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.43\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.04\n",
      "-------------------------\n",
      "Iterations 84 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 85 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.43\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 86 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.56\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 87 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 88 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 89 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 90 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.92\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 91 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 92 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 93 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.46\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 94 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 95 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.03\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 96 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.27\n",
      "-------------------------\n",
      "Iterations 97 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.49\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 98 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 99 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.47\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.04\n",
      "-------------------------\n",
      "Iterations 100 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.74\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 101 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.44\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 102 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.03\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 103 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 104 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 105 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 106 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 107 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 108 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.24\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 109 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 110 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 111 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.94\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 112 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.74\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 113 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 114 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 115 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.14\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 116 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 117 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 118 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.78\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 119 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.62\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 120 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.62\n",
      "Final KL: 0.10\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 121 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.69\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 122 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.81\n",
      "Mean Policy Loss: 0.03\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 123 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.77\n",
      "Final KL: 0.13\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 124 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 125 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 126 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 127 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 128 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 129 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.43\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.04\n",
      "-------------------------\n",
      "Iterations 130 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.81\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.19\n",
      "-------------------------\n",
      "Iterations 131 Complete\n",
      "Epochs 9 \n",
      "Mean Rtg: -0.85\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 132 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.04\n",
      "-------------------------\n",
      "Iterations 133 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 134 Complete\n",
      "Epochs 8 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 135 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 136 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.83\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 137 Complete\n",
      "Epochs 9 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 138 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.01\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 139 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.49\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 140 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.35\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 141 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 142 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 143 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 144 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.70\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 145 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.47\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 146 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 147 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 148 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 149 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 150 Complete\n",
      "Epochs 9 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 151 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.15\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 152 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 153 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.75\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.15\n",
      "-------------------------\n",
      "Iterations 154 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 155 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 156 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.74\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 157 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.10\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 158 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 159 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.11\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 160 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 161 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 162 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.38\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 163 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 1.19\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 164 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 165 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.12\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 166 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 167 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.49\n",
      "Final KL: 0.17\n",
      "Mean Policy Loss: 4.39\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 168 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 169 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 170 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.54\n",
      "Final KL: 1445.53\n",
      "Mean Policy Loss: 0.10\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 171 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.44\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 172 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 173 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.47\n",
      "Final KL: 0.02\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 174 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.29\n",
      "-------------------------\n",
      "Iterations 175 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 176 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.51\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 177 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.41\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 178 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.18\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 179 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.72\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 180 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.69\n",
      "Final KL: 0.12\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 181 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 182 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.36\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 183 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 184 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.36\n",
      "Final KL: 0.25\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.05\n",
      "-------------------------\n",
      "Iterations 185 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.16\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 186 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.80\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 187 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.37\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 188 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.15\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 189 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.15\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 190 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.06\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 191 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 192 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.44\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 193 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 194 Complete\n",
      "Epochs 7 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 195 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 196 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.13\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 197 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.53\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.17\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 198 Complete\n",
      "Epochs 6 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 199 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.60\n",
      "Final KL: 0.03\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 200 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.85\n",
      "Final KL: 0.28\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 201 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.56\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 202 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.76\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.27\n",
      "-------------------------\n",
      "Iterations 203 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.45\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 204 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.86\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.19\n",
      "-------------------------\n",
      "Iterations 205 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 206 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.42\n",
      "Final KL: 0.74\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 207 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 208 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.13\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 209 Complete\n",
      "Epochs 7 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 210 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 211 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.73\n",
      "Final KL: 0.20\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 212 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.52\n",
      "Final KL: 0.10\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 213 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 1.06\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 214 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.94\n",
      "Mean Policy Loss: 0.08\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 215 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.09\n",
      "Mean Policy Loss: 0.34\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 216 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.63\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 217 Complete\n",
      "Epochs 5 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 218 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 219 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.23\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 220 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 221 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.16\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 222 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 223 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.11\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 224 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.77\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 225 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.50\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 226 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 227 Complete\n",
      "Epochs 8 \n",
      "Mean Rtg: -0.66\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 228 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.62\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 229 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.74\n",
      "Final KL: 0.26\n",
      "Mean Policy Loss: 0.03\n",
      "Mean Value Loss: 0.13\n",
      "-------------------------\n",
      "Iterations 230 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.12\n",
      "-------------------------\n",
      "Iterations 231 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 232 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.03\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 233 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.56\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 234 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.64\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 235 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.05\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 236 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.35\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 237 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 10.64\n",
      "Mean Policy Loss: 2.02\n",
      "Mean Value Loss: 0.17\n",
      "-------------------------\n",
      "Iterations 238 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.61\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.07\n",
      "-------------------------\n",
      "Iterations 239 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.68\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 240 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.14\n",
      "-------------------------\n",
      "Iterations 241 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.55\n",
      "Final KL: 0.13\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 242 Complete\n",
      "Epochs 10 \n",
      "Mean Rtg: -0.72\n",
      "Final KL: 0.04\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.18\n",
      "-------------------------\n",
      "Iterations 243 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.18\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 244 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.84\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.16\n",
      "-------------------------\n",
      "Iterations 245 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.75\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 246 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.48\n",
      "Final KL: 0.07\n",
      "Mean Policy Loss: 0.12\n",
      "Mean Value Loss: 0.10\n",
      "-------------------------\n",
      "Iterations 247 Complete\n",
      "Epochs 4 \n",
      "Mean Rtg: -0.71\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.01\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 248 Complete\n",
      "Epochs 2 \n",
      "Mean Rtg: -0.58\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.01\n",
      "Mean Value Loss: 0.06\n",
      "-------------------------\n",
      "Iterations 249 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.67\n",
      "Final KL: 0.37\n",
      "Mean Policy Loss: 0.02\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 250 Complete\n",
      "Epochs 1 \n",
      "Mean Rtg: -0.57\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: 0.03\n",
      "Mean Value Loss: 0.09\n",
      "-------------------------\n",
      "Iterations 251 Complete\n",
      "Epochs 3 \n",
      "Mean Rtg: -0.59\n",
      "Final KL: 0.08\n",
      "Mean Policy Loss: 0.00\n",
      "Mean Value Loss: 0.11\n",
      "-------------------------\n",
      "Iterations 252 Complete\n",
      "Epochs 7 \n",
      "Mean Rtg: -0.65\n",
      "Final KL: 0.06\n",
      "Mean Policy Loss: -0.00\n",
      "Mean Value Loss: 0.08\n",
      "-------------------------\n",
      "Iterations 253 Complete\n",
      "Epochs 2 \n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      "  [1] try_yieldto(undo::typeof(Base.ensure_rescheduled))",
      "    @ Base .\\task.jl:931",
      "  [2] wait()",
      "    @ Base .\\task.jl:995",
      "  [3] uv_write(s::Base.PipeEndpoint, p::Ptr{UInt8}, n::UInt64)",
      "    @ Base .\\stream.jl:1048",
      "  [4] unsafe_write(s::Base.PipeEndpoint, p::Ptr{UInt8}, n::UInt64)",
      "    @ Base .\\stream.jl:1120",
      "  [5] unsafe_write",
      "    @ .\\io.jl:431 [inlined]",
      "  [6] unsafe_write",
      "    @ .\\io.jl:698 [inlined]",
      "  [7] write",
      "    @ .\\io.jl:721 [inlined]",
      "  [8] format(io::IJulia.IJuliaStdio{Base.PipeEndpoint}, f::Printf.Format{Base.CodeUnits{UInt8, String}, Tuple{Printf.Spec{Val{'i'}}}}, args::Int64)",
      "    @ Printf C:\\Users\\Bikrant\\AppData\\Local\\julias\\julia-1.10\\share\\julia\\stdlib\\v1.10\\Printf\\src\\Printf.jl:934",
      "  [9] PPO(env::PretrainingGateEnv; trajectory_batch_size::Int64, iterations::Int64, initial_policy::Nothing, l::Vector{Int64}, epochs::Int64, η::Float32, fit_batch_size::Int64, ϵ::Float32, verbose::Bool, γ::Float32, λ::Float32, KL_targ::Float32, clip_grad_tresh::Float32, vf_ratio::Float32, trajectory_kwargs::@Kwargs{})",
      "    @ Main C:\\Users\\Bikrant\\PicoRL\\RL-Copy1.jl:453",
      " [10] top-level scope",
      "    @ In[12]:1"
     ]
    }
   ],
   "source": [
    "ACpolicy = PPO(Pretraining_Env;iterations=250,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f87688e8-f92e-4ed4-9b3d-c9fb51114381",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `ACpolicy` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `ACpolicy` not defined",
      "",
      "Stacktrace:",
      " [1] (::var\"#184#185\")(v::Float64)",
      "   @ Main .\\none:0",
      " [2] iterate",
      "   @ .\\generator.jl:47 [inlined]",
      " [3] collect(itr::Base.Generator{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}, var\"#184#185\"})",
      "   @ Base .\\array.jl:834",
      " [4] top-level scope",
      "   @ In[13]:2"
     ]
    }
   ],
   "source": [
    "x = range(0,2*pi,1000)\n",
    "y = [ACpolicy.std_network([v])[1] for v in x]\n",
    "using CairoMakie\n",
    "lines(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d54116d9-d477-4f7b-a8cd-f14438f90db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Float64}:\n",
       " 2.5132741228718345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.reset!(Pretraining_Env,angle = [range(0,2*pi,11)[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d1e114e-8b4c-4b9d-8e4e-0bb612279cf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `ACpolicy` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `ACpolicy` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[15]:1"
     ]
    }
   ],
   "source": [
    "rewards,acts,states =SampleTrajectory(ACpolicy,Pretraining_Env;deterministic=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d055d59-4f43-4780-a2aa-a5f359f703d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×51 Matrix{Float64}:\n",
       " 0.0  1.83246e-20  -0.0196885    -0.0507178   …  0.0708192  0.0285495   0.0\n",
       " 0.0  6.10707e-20   0.000972287   0.00170063     0.0179483  0.00665824  0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RZ_traj[:a5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87fd2310-a116-4031-a0aa-06ce20fe0002",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: `Figure` not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: `Figure` not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[17]:1"
     ]
    }
   ],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1])\n",
    "lines!(ax,1:n_steps,Pretraining_Env.a[1,:])\n",
    "lines!(ax,1:n_steps,RZ_traj[:a5][1,:])\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.4",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
